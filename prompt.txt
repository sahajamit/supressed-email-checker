I want to build a simple api server in pythin using FAST API.
It will expose a simple rest api, could be post call which will ask from the user about an email id and will return the result if the email id is supressed or not.

The api will look for a json file at given path locally which is confugurable and then this api has to return a response json object to tell if the email is supressed or not via boolean value and if it is suppressed it will return the reason and last update time. Above this it will also return a synthesise answer in proper english language that the given email is supresses with human readable time in timezone and elborate the reason in simple english.

This last bit of elaboration in english should be done by ollama api so where we will run any local model and make a call to it. The model details will be defined in a configuration so this python code will use ollama python sdk the latest version and then use stream = false and get the final answer in english abut the supressed email. We have to draft a proper promt for ollama model so that it only returns the final answer in english and not any other text.

Here is the sample json which has the info about supressed emails.

{
    "SuppressedDestinationSummaries": [
        {
            "EmailAddress": "recipient2@example.com",
            "Reason": "COMPLAINT",
            "LastUpdateTime": "2020-04-10T21:03:05Z"
        },
        {
            "EmailAddress": "recipient0@example.com",
            "Reason": "COMPLAINT",
            "LastUpdateTime": "2020-04-10T21:04:26Z"
        },
        {
            "EmailAddress": "recipient1@example.com",
            "Reason": "BOUNCE",
            "LastUpdateTime": "2020-04-10T22:07:59Z"
        }
    ]
}

I have already running ollama on my machine and it has this model: qwen3:8b and you have to disable the thinking mode to get the answer to make it more fast. Latest ollama sdk has a aparamter to disable thinking mode for the models which cna think and for the model which cannot think it should ignore it.
Here is the ollama sdk version we have to use.
pip install ollama  0.5.1

example code to disable thinking mode:

from ollama import chat

messages = [
  {
    'role': 'user',
    'content': 'What is 10 + 23?',
  },
]

response = chat('deepseek-r1', messages=messages, think=True)

print('Thinking:\n========\n\n' + response.message.thinking)
print('\nResponse:\n========\n\n' + response.message.content)